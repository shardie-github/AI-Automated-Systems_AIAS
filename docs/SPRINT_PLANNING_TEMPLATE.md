# Sprint Planning Template - 30-Day Sprint

**Sprint Number:** [N+1]  
**Sprint Start Date:** [YYYY-MM-DD]  
**Sprint End Date:** [YYYY-MM-DD + 30 days]  
**Status:** ðŸŽ¯ Planning

---

## 1. SPRINT GOAL

### Goal Statement
**By the end of this 30-day sprint, [USER] can [ACTION] and we can measure [METRIC].**

**Example:**
> By the end of this 30-day sprint, a user can deploy one pre-built AI agent and have it responding to queries via chat interface. We can measure deployments and active conversations.

### Success Criteria (3-5 Measurable Outcomes)

1. âœ… **[Outcome 1]** - Measurable: [How we measure]
2. âœ… **[Outcome 2]** - Measurable: [How we measure]
3. âœ… **[Outcome 3]** - Measurable: [How we measure]
4. âœ… **[Learning Outcome]** - Measurable: [How we validate]
5. âœ… **[Technical Outcome]** - Measurable: [How we verify]

**Example:**
1. âœ… Users can deploy agents - Measurable: Deployment API creates records, telemetry event fires
2. âœ… Agents respond to queries - Measurable: Chat API returns OpenAI responses, response time <2s
3. âœ… We can measure deployments - Measurable: Dashboard shows deployment count
4. âœ… We learn user behavior - Measurable: 5 user testing sessions completed, feedback documented
5. âœ… System is reliable - Measurable: Error rate <1%, uptime >99%

---

## 2. WEEKLY MILESTONES

### Week 1: [Foundation/Architecture]
**Goal:** [What must be done by end of week]

**Must Complete:**
- [ ] [Critical task 1]
- [ ] [Critical task 2]
- [ ] [Critical task 3]

**Checkpoint Criteria:**
- [ ] [Measurable outcome 1]
- [ ] [Measurable outcome 2]

**Demo Script:**
1. [Step 1]
2. [Step 2]
3. [Step 3]

---

### Week 2: [Core Functionality]
**Goal:** [What must be done by end of week]

**Must Complete:**
- [ ] [Critical task 1]
- [ ] [Critical task 2]
- [ ] [User validation task]

**Checkpoint Criteria:**
- [ ] [Measurable outcome 1]
- [ ] [Measurable outcome 2]
- [ ] [User feedback captured]

**Demo Script:**
1. [Step 1]
2. [Step 2]
3. [Step 3]

---

### Week 3: [Hardening/Observability]
**Goal:** [What must be done by end of week]

**Must Complete:**
- [ ] [Reliability task]
- [ ] [Observability task]
- [ ] [User testing task]

**Checkpoint Criteria:**
- [ ] [Performance target met]
- [ ] [Error tracking working]
- [ ] [User feedback synthesized]

**Demo Script:**
1. [Step 1]
2. [Step 2]
3. [Step 3]

---

### Week 4: [Polish/Validation]
**Goal:** [What must be done by end of week]

**Must Complete:**
- [ ] [Polish task]
- [ ] [Metrics dashboard]
- [ ] [Sprint retrospective]

**Checkpoint Criteria:**
- [ ] [All success criteria met]
- [ ] [Metrics visible]
- [ ] [Learning documented]

**Demo Script:**
1. [Step 1]
2. [Step 2]
3. [Step 3]

---

## 3. SPRINT BACKLOG

### Backend Tasks

#### Week 1
**Task B1.1: [Task Name]** [Size: S/M/L]
- **Summary:** [What this task does]
- **Acceptance Criteria:**
  - [ ] [Criterion 1 - includes metrics]
  - [ ] [Criterion 2 - includes measurement]
  - [ ] [Criterion 3 - includes validation]
- **Files:** `[file/path]`
- **Dependencies:** [Task dependencies]
- **Learning Question:** [What question does this answer?]

---

### Frontend Tasks

#### Week 1
**Task F1.1: [Task Name]** [Size: S/M/L]
- **Summary:** [What this task does]
- **Acceptance Criteria:**
  - [ ] [Criterion 1 - includes metrics]
  - [ ] [Criterion 2 - includes measurement]
  - [ ] [Criterion 3 - includes validation]
- **Files:** `[file/path]`
- **Dependencies:** [Task dependencies]
- **Learning Question:** [What question does this answer?]

---

### Data / Analytics / Telemetry Tasks

#### Week 1
**Task D1.1: [Task Name]** [Size: S/M/L]
- **Summary:** [What this task does]
- **Acceptance Criteria:**
  - [ ] [Criterion 1 - includes metrics]
  - [ ] [Criterion 2 - includes measurement]
  - [ ] [Criterion 3 - includes validation]
- **Files:** `[file/path]`
- **Dependencies:** [Task dependencies]
- **Learning Question:** [What question does this answer?]

---

### Validation Tasks

#### Week 2
**Task V2.1: [User Testing Session]** [Size: M]
- **Summary:** Run user testing session with 3-5 users
- **Acceptance Criteria:**
  - [ ] 3-5 users complete testing session
  - [ ] Feedback documented in `docs/sprint-learnings/`
  - [ ] Action items created from feedback
- **Files:** `docs/sprint-learnings/week-2-user-testing.md`
- **Dependencies:** [Feature tasks that need testing]
- **Learning Question:** Can users [use feature] without help? What friction exists?

---

## 4. RISK MITIGATION

### Technical Risks

**Risk 1: [Risk Name]**
- **Mitigation:** [How we prevent]
- **Contingency:** [What we do if it happens]

---

### Product Risks

**Risk 1: [Risk Name]**
- **Mitigation:** [How we prevent]
- **Contingency:** [What we do if it happens]

---

## 5. VALIDATION & FEEDBACK LOOP

### Validation Activities

#### Activity 1: [Activity Name]
- **When:** [Day/Week]
- **What We Show:** [What users see]
- **What We Measure:** [Metrics]
- **Success Bar:** [Success criteria]

---

### Feedback Digestion

**Artifacts Created:**
- `docs/sprint-learnings/week-[N]-[activity].md` - [Activity] feedback synthesis

**Feedback Translation Process:**
1. Categorize feedback (critical bugs, UX improvements, feature requests)
2. Create follow-up issues (GitHub issues tagged `[sprint-name]`, `post-sprint`)
3. Update documentation (user guides, API docs)

---

## 6. SUCCESS METRICS & TRACKING

### Weekly Metrics Dashboard

**Week 1:**
- [Metric 1]: [Target]
- [Metric 2]: [Target]
- [Metric 3]: [Target]

**Week 2:**
- [Metric 1]: [Target]
- [Metric 2]: [Target]
- [Metric 3]: [Target]

**Week 3:**
- [Metric 1]: [Target]
- [Metric 2]: [Target]
- [Metric 3]: [Target]

**Week 4:**
- [Metric 1]: [Target]
- [Metric 2]: [Target]
- [Metric 3]: [Target]

---

## 7. SPRINT RETROSPECTIVE QUESTIONS

At end of sprint, answer:

1. What went well?
2. What could be improved?
3. What did we learn?
4. What should we do differently next sprint?
5. Did we achieve our sprint goal?

---

## APPENDIX: QUICK REFERENCE

### Key Files to Know
- **Database:** `supabase/migrations/`
- **API Routes:** `app/api/`
- **Components:** `components/`
- **Telemetry:** `lib/telemetry/track.ts`

### Key Commands
```bash
pnpm dev              # Start dev server
pnpm typecheck        # Type check
pnpm lint             # Lint code
pnpm test             # Run tests
pnpm build            # Build for production
```

### Key Environment Variables
- `SUPABASE_URL` - Supabase project URL
- `SUPABASE_ANON_KEY` - Supabase anon key
- `OPENAI_API_KEY` - OpenAI API key (if using)

---

**Document Status:** âœ… Template Ready  
**Last Updated:** 2025-01-29  
**Next Review:** End of Sprint
