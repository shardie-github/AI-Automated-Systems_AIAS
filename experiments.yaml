# Pricing & Offer Experiments — AIAS Platform
# Generated by: Experiment Orchestrator
# Date: 2025-01-31
# Input: pricing_models.yaml + investor_narrative.yaml

# C1. Pricing & Offer Experiments
experiments:
  - name: "Price Point Test (Starter Plan)"
    id: "exp_price_starter"
    hypothesis: |
      Lower price ($39/month) will increase signup rate but may reduce perceived 
      value. Higher price ($59/month) may reduce signup but increase ARPU and 
      signal quality. Current $49/month may be optimal balance.
    variants:
      - name: "Variant A: Lower Price"
        price: "$39/month"
        description: "More accessible, may attract price-sensitive users"
      - name: "Variant B: Current Price (Control)"
        price: "$49/month"
        description: "Current pricing, baseline for comparison"
      - name: "Variant C: Higher Price"
        price: "$59/month"
        description: "Premium positioning, may signal higher value"
    target_segment: "Solo e-commerce operators, independent consultants"
    where_appears:
      - "Pricing page (primary)"
      - "Landing page CTA"
      - "Email campaigns"
    implementation: "Feature flag or A/B test in Stripe checkout"
    duration: "30 days minimum, 60 days preferred"
    sample_size: "100+ signups per variant (300+ total)"

  - name: "Free Tier vs Limited-Time Trial"
    id: "exp_free_tier"
    hypothesis: |
      Free tier (3 agents, 100 automations) may increase signup but reduce 
      conversion to paid. Limited-time trial (14 days, full features) may 
      increase conversion but reduce signup volume.
    variants:
      - name: "Variant A: Free Tier (Current)"
        offer: "Free plan: 3 agents, 100 automations/month, forever"
        description: "Low barrier to entry, may reduce urgency to upgrade"
      - name: "Variant B: 14-Day Trial"
        offer: "14-day free trial, full Starter features, then $49/month"
        description: "Creates urgency, may increase conversion but reduce signup"
      - name: "Variant C: Hybrid (Free + Trial)"
        offer: "Free plan + option for 14-day trial of paid plans"
        description: "Best of both worlds, may optimize for both signup and conversion"
    target_segment: "All new users"
    where_appears:
      - "Signup flow"
      - "Landing page"
      - "Pricing page"
    implementation: "User segmentation at signup, track conversion funnel"
    duration: "60 days minimum"
    sample_size: "200+ signups per variant (600+ total)"

  - name: "Value Metric Presentation (Per-Agent vs Per-Automation)"
    id: "exp_value_metric"
    hypothesis: |
      Presenting pricing as "per agent" may be more intuitive and align with 
      product positioning. Presenting as "per automation" may better reflect 
      actual usage but could create bill shock concerns.
    variants:
      - name: "Variant A: Agent-Focused (Current)"
        presentation: "10 AI agents, unlimited automations"
        description: "Emphasizes capabilities, aligns with product positioning"
      - name: "Variant B: Automation-Focused"
        presentation: "Unlimited automations, up to 10 agents"
        description: "Emphasizes usage, may better reflect value"
      - name: "Variant C: Outcome-Focused"
        presentation: "Save 10+ hours/week, 10 AI agents included"
        description: "Emphasizes ROI, may increase perceived value"
    target_segment: "All users viewing pricing page"
    where_appears:
      - "Pricing page"
      - "Feature comparison table"
    implementation: "A/B test pricing page copy, track engagement and conversion"
    duration: "30 days"
    sample_size: "500+ page views per variant (1,500+ total)"

  - name: "Annual Discount Test (10% vs 20% vs No Discount)"
    id: "exp_annual_discount"
    hypothesis: |
      Annual discount increases cash flow and reduces churn risk. 20% discount 
      (current) may be optimal, but 10% may still drive annual signups while 
      preserving more revenue.
    variants:
      - name: "Variant A: 10% Annual Discount"
        offer: "$529/year (save $59, 10% off)"
        description: "Smaller discount, preserves more revenue"
      - name: "Variant B: 20% Annual Discount (Current)"
        offer: "$490/year (save $98, 20% off)"
        description: "Larger discount, may increase annual signups"
      - name: "Variant C: No Annual Discount"
        offer: "$588/year (monthly × 12, no discount)"
        description: "Baseline, tests if discount is necessary"
    target_segment: "Users considering Starter or Pro plans"
    where_appears:
      - "Pricing page"
      - "Checkout flow"
      - "Email campaigns (renewal reminders)"
    implementation: "Stripe pricing tiers, track annual vs monthly signups"
    duration: "90 days (capture annual signup patterns)"
    sample_size: "50+ annual signups per variant (150+ total)"

  - name: "Done-for-You Onboarding vs Self-Serve"
    id: "exp_onboarding"
    hypothesis: |
      Done-for-you onboarding (guided setup call) may increase activation and 
      conversion but requires sales resources. Self-serve may scale better but 
      may reduce activation.
    variants:
      - name: "Variant A: Self-Serve (Current)"
        offer: "Guided wizard, email support, community forum"
        description: "Scalable, low-touch, may reduce activation"
      - name: "Variant B: Done-for-You Onboarding"
        offer: "Free 30-minute setup call, workflow deployment assistance"
        description: "High-touch, may increase activation and conversion"
      - name: "Variant C: Hybrid (Tiered)"
        offer: "Self-serve for Free/Starter, done-for-you for Pro+"
        description: "Optimizes resources, may increase Pro conversions"
    target_segment: "New signups (all tiers or Pro+ only)"
    where_appears:
      - "Post-signup email sequence"
      - "In-app onboarding flow"
      - "Pricing page (for Pro+ plans)"
    implementation: "Email automation + calendar booking, track activation rates"
    duration: "60 days"
    sample_size: "100+ signups per variant (300+ total)"

  - name: "Feature Gating Test (Starter vs Pro)"
    id: "exp_feature_gating"
    hypothesis: |
      More restrictive Starter plan (fewer agents, limited templates) may 
      drive upgrades to Pro. Less restrictive Starter (more agents, all templates) 
      may increase Starter signups but reduce Pro upgrades.
    variants:
      - name: "Variant A: Restrictive Starter (Current)"
        starter_features: "10 agents, 50+ templates, basic analytics"
        pro_features: "50 agents, all templates, advanced analytics, API"
        description: "Clear upgrade path, may drive Pro conversions"
      - name: "Variant B: Generous Starter"
        starter_features: "20 agents, all templates, advanced analytics"
        pro_features: "50 agents, all templates, API, white-label, priority support"
        description: "More value in Starter, may increase Starter signups"
    target_segment: "Users comparing Starter vs Pro"
    where_appears:
      - "Pricing page feature comparison"
      - "In-app upgrade prompts"
    implementation: "Feature flags, track Starter vs Pro signups and upgrades"
    duration: "60 days"
    sample_size: "200+ signups per variant (400+ total)"

# C2. Success Metrics & Guardrails
success_metrics:
  exp_price_starter:
    primary_metrics:
      - "Signup rate (visitors → signups)"
      - "Trial-to-paid conversion rate"
      - "ARPU (average revenue per user)"
      - "LTV (lifetime value)"
    guardrail_metrics:
      - "Activation rate (users who deploy first workflow)"
      - "Support load (tickets per user)"
      - "NPS (Net Promoter Score)"
      - "Churn rate (monthly)"
    success_criteria: |
      Higher price variant should have ≥5% higher ARPU and LTV without 
      reducing signup rate by >20%. Lower price variant should have ≥20% 
      higher signup rate without reducing ARPU by >10%.

  exp_free_tier:
    primary_metrics:
      - "Signup rate (visitors → signups)"
      - "Free-to-paid conversion rate"
      - "Time to first paid conversion"
      - "ARPU (from converted users)"
    guardrail_metrics:
      - "Activation rate (free users who deploy workflow)"
      - "Support load (free users vs paid)"
      - "Churn rate (free users who never convert)"
    success_criteria: |
      Trial variant should have ≥10% higher free-to-paid conversion without 
      reducing signup rate by >15%. Free tier variant should have ≥30% higher 
      signup rate without reducing conversion by >5%.

  exp_value_metric:
    primary_metrics:
      - "Pricing page engagement (time on page, scroll depth)"
      - "Signup rate (from pricing page)"
      - "Trial-to-paid conversion"
      - "Feature usage (agents vs automations)"
    guardrail_metrics:
      - "Support questions about pricing"
      - "Bounce rate from pricing page"
    success_criteria: |
      Outcome-focused variant should have ≥10% higher engagement and conversion. 
      Agent-focused variant should align with product usage patterns.

  exp_annual_discount:
    primary_metrics:
      - "Annual signup rate (% of signups choosing annual)"
      - "Cash flow (upfront revenue)"
      - "Churn rate (annual vs monthly)"
      - "LTV (annual customers)"
    guardrail_metrics:
      - "Refund requests (annual customers)"
      - "Support load"
    success_criteria: |
      20% discount should drive ≥30% annual signup rate. 10% discount should 
      preserve ≥15% more revenue while still driving ≥20% annual signup rate.

  exp_onboarding:
    primary_metrics:
      - "Activation rate (users who deploy first workflow)"
      - "Time to activation (days from signup)"
      - "Trial-to-paid conversion"
      - "ARPU (from onboarded users)"
    guardrail_metrics:
      - "Support load (onboarding calls)"
      - "Sales capacity (calls per week)"
      - "NPS (from onboarded users)"
    success_criteria: |
      Done-for-you variant should have ≥20% higher activation and ≥15% higher 
      conversion. Hybrid variant should optimize for Pro conversions without 
      overwhelming sales capacity.

  exp_feature_gating:
    primary_metrics:
      - "Starter vs Pro signup mix"
      - "Starter-to-Pro upgrade rate"
      - "ARPU (weighted average)"
      - "Feature usage (which features drive upgrades)"
    guardrail_metrics:
      - "Starter churn rate"
      - "Support questions about feature limits"
    success_criteria: |
      Restrictive Starter should drive ≥10% Starter-to-Pro upgrades without 
      increasing Starter churn by >5%. Generous Starter should increase Starter 
      signups by ≥20% without reducing Pro signups by >10%.

# C3. Implementation Guidelines
implementation:
  feature_flags:
    tool: "LaunchDarkly, Split.io, or custom feature flag system"
    approach: |
      Create feature flags for each experiment variant. Route users to variants 
      based on user ID hash (consistent assignment). Track variant assignment 
      in analytics.
    example: |
      if (featureFlag('exp_price_starter') === 'variant_a') {
        price = 39;
      } else if (featureFlag('exp_price_starter') === 'variant_b') {
        price = 49; // control
      } else {
        price = 59;
      }

  analytics_events:
    required_events:
      - name: "PricingPageViewed"
        properties: ["variant", "user_segment", "referrer"]
      - name: "PlanSelected"
        properties: ["plan_name", "price", "billing_period", "variant"]
      - name: "CheckoutStarted"
        properties: ["plan_name", "price", "variant"]
      - name: "CheckoutCompleted"
        properties: ["plan_name", "price", "user_id", "variant"]
      - name: "TrialActivated"
        properties: ["plan_name", "variant", "user_id"]
      - name: "TrialConverted"
        properties: ["plan_name", "conversion_date", "variant", "user_id"]
      - name: "WorkflowDeployed"
        properties: ["workflow_id", "user_id", "variant", "days_since_signup"]
      - name: "UpgradePrompted"
        properties: ["from_plan", "to_plan", "variant", "user_id"]
      - name: "UpgradeCompleted"
        properties: ["from_plan", "to_plan", "variant", "user_id"]
    tracking_tool: "Mixpanel, Amplitude, or PostHog"
    implementation: |
      Track all events with variant assignment. Use consistent user IDs across 
      sessions. Store experiment assignments in user profile for analysis.

  sample_size_calculations:
    methodology: |
      Use statistical power analysis (80% power, 95% confidence). Minimum 
      detectable effect: 10-20% difference between variants. Account for 
      multiple comparisons (Bonferroni correction if testing multiple variants).
    tools: |
      - Evan Miller's Sample Size Calculator
      - Optimizely Sample Size Calculator
      - Custom power analysis script
    minimum_sample_sizes:
      pricing_experiments: "100+ conversions per variant"
      conversion_experiments: "200+ signups per variant"
      engagement_experiments: "500+ page views per variant"

  duration_recommendations:
    pricing_experiments: |
      Minimum 30 days to capture full billing cycle. Preferred 60 days to 
      account for seasonality and learning effects.
    conversion_experiments: |
      Minimum 60 days to capture trial-to-paid conversion (14-day trial + 
      decision time). Preferred 90 days for annual signup patterns.
    engagement_experiments: |
      Minimum 14 days to capture user behavior patterns. Preferred 30 days 
      for statistical significance.

  analysis_framework:
    statistical_tests: |
      - Chi-square test for conversion rates
      - T-test for continuous metrics (ARPU, LTV)
      - Survival analysis for churn/time-to-conversion
      - Bayesian analysis for early stopping (optional)
    reporting: |
      Weekly experiment dashboards. Statistical significance thresholds: 
      p < 0.05 for primary metrics, p < 0.10 for guardrail metrics. 
      Document learnings and next steps.
    decision_criteria: |
      - Statistical significance (p < 0.05)
      - Practical significance (≥10% improvement)
      - Guardrail metrics not harmed
      - Business judgment (strategic fit)

  rollout_strategy:
    phase_1: |
      Start with low-risk experiments (value metric presentation, annual 
      discount). Build confidence and learnings.
    phase_2: |
      Run pricing experiments with smaller sample sizes first. Monitor closely 
      for negative impacts.
    phase_3: |
      Scale winning variants to 100% of traffic. Iterate on learnings.
    rollback_plan: |
      If guardrail metrics are harmed (churn >10%, NPS <30), rollback 
      immediately. Have feature flags ready for instant rollback.
